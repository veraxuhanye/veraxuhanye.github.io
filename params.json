{
  "name": "Hanye Xu GitHub Page",
  "tagline": "Linkedin: https://www.linkedin.com/in/hanye-xu",
  "body": "Welcome to my GitHub page! -- Hanye\r\n\r\nMetricsVis Project\r\n![Overview Of MetricsVis](https://veraxuhanye.github.io/MetricsVis/images/overview.png)\r\n[MetricsVis Demo](https://veraxuhanye.github.io/MetricsVis/)\r\n\r\n\r\n***\r\n\r\nWork Logs:\r\n* **10/6 - 10/14**\r\n      - Finished Weibo login python code: The Weibo login code take in those three weibo accounts' usernames and passwords try to login into to Weibo. Since Weibo encode usernames and passwords, we need to encode the username and password into \"Weibo\" form. Then, a postdata for login have to be completed for login. After sending the request for login, we need to check the returning html to see if it is successful. More details are included in comments with the code. \r\n      - Added verification picture type in function to the code: Since the three weibo account we obtained cannot login normally, a python function is written to obtain the verification picture from Weibo login page. The user needs to manually type in verification code for login. There are 5 changes for each account to try the verification code, otherwise, the login will fail. \r\n* **10/14 - 10/21**\r\n      - Finished Baidu News Search crawler code: Get news items from Baidu News Search and stored them in csv file with title, time, author, content and url links. Instead of using an existing API, the code is written to getting the news from news.baidu.com. In the process, we first encode the search keyword into the url for news.baidu.com. Then parse the whole html. For Baidu.com, it provides the list of articles with title, time, author and url in their seach page. Reg expression functions are written to obtain the information. Next, a function is written to get access to the url (the news article website), and also use libs and regular expressions to obtain the context of those article. The lib I used is called beautifulSoup.\r\n      - Additionally, a text file is used for storing all these visited url to prevent duplication. All these information are stored in a cvs file.\r\n* _TODO:_ \r\n      - some page cannot be read, messy code Chinese shown\r\n      - the csv file cannot show Chinese properly, but google drive is able to show\r\n\r\n\r\n* **10/22 - 10/28**\r\n      - Finished Google News Search crawler code: Get news items from Google News Search and stored them in a csv file with title, time, author, content and url links. The API I used here is called gnp. Unlike Baidue news, the news.google.com only provide one page of search result without sorting. After sending request using gnp, a json file returned as a search result. Next, a function is written to parse the json file. In the json file, only title, author, url are provided. The same method is used to get the full content of the news article as Baidu part. Then store information in a csv file. \r\n      - PDF generate function is also implemented to both news crawler. The lib I used here is called pdfkit with wkhtmltopdf. This lib is a little bit slow, so I modify it to be generating pdf without pictures and links. I also tried another lib, however, it can convert web to png. \r\n* _TODO:_\r\n      - Try another lib if it can convert web to pdf\r\n      - Get time from google news article \r\n\r\n* **10/29 - 11/04**\r\n      - What have been done so far:\r\n      - finished integrating keyword list into the baidu news crawler, each keyword has 100 related news, however, it takes about 1 hour to generate 100 news' pdf???!!! Have to think about another way....\r\n      - finished cvs file modify, each row has a pdf num, category, keyword followed by previous content.\r\n      - finished getting publish date of google news articles\r\n      - Changed a html pdf lib for generating pdfs from an url. (aprocximately 30-40min for 100 articles)\r\n* _TODO:_   \r\n       - optimize pdf generation process, or change a lib \r\n       - Q: Does keywords have unique news ? or duplicate news can be exist among all??\r\n       - Q: All info save to one csv or seperate?? \r\n       - Q: 100 news for baidue almost covers more than one month's news, do we shrink it?",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}